import requests
import feedparser
from datetime import datetime

def test_rss(url, max_items=3):
    """æµ‹è¯•RSSæºæ˜¯å¦å¯ç”¨ï¼Œå¹¶æ‰“å°æ ‡å‡†æ ¼å¼è¾“å‡º"""
    print(f"\n=== ğŸ§ª æµ‹è¯• RSS æº: {url} ===")

    headers = {"User-Agent": "Mozilla/5.0 (compatible; RSSTester/1.0; +https://example.com)"}

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        print("ğŸ“„ Content-Type:", resp.headers.get("Content-Type", "æœªçŸ¥"))
        print("ğŸŒ çŠ¶æ€ç :", resp.status_code)
        if resp.status_code != 200:
            print("âŒ è¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®æˆ–æºæ˜¯å¦è¢«å±è”½ã€‚")
            return

        feed = feedparser.parse(resp.text)
        entries = feed.entries
        print(f"âœ… æˆåŠŸè§£æï¼Œå…± {len(entries)} æ¡")
        if not entries:
            print("âš ï¸ è§£æä¸ºç©ºï¼Œå¯èƒ½éœ€è¦å¸¦UAè®¿é—®æˆ–ä¸æ˜¯æ ‡å‡†RSSæ ¼å¼ã€‚")
            return

        # æ‰“å°å‰å‡ æ¡æ ‡å‡†åŒ–è¾“å‡º
        print("\n=== ç¤ºä¾‹è¾“å‡º ===")
        for i, entry in enumerate(entries[:max_items], 1):
            title = entry.get("title", "").strip()
            summary = entry.get("summary", "").strip()
            link = entry.get("link", "").strip()

            pub_date = entry.get("published") or entry.get("updated") or ""
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M")

            print(f"\n{i}. {title}\nğŸ—“ï¸ {pub_date}\n{summary[:100]}...\nğŸ”— {link}")

    except Exception as e:
        print(f"âŒ æŠ“å–å‡ºé”™: {e}")


if __name__ == "__main__":
    # ğŸ‘‰ åœ¨è¿™é‡Œä¸´æ—¶å¡«å…¥ä½ æƒ³æµ‹è¯•çš„æ–° RSS æº
    test_urls = [
        "https://rsshub.app/aibase/news",
        # ä¾‹å¦‚:
        # "https://techcrunch.com/tag/ai/feed/",
        # "https://wechat2rss.xlab.app/feed/xxxxxx.xml",
    ]

    for url in test_urls:
        test_rss(url)
